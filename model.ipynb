{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mP7v7FJWy-Jr"
   },
   "source": [
    "## Dataset & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTFH0NMFu4sH",
    "outputId": "903103b0-b9b2-4af4-faae-5e6e2a770820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /usr/local/anaconda3/lib/python3.9/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcLSkd74tq6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2% [.                                                 ]  15327232 / 588219670"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "# The dataset we used belongs to Leaf Segmentation and Counting Challenge (LCC and LSC). \n",
    "# It is publicly available at https://www.plant-phenotyping.org/CVPPP2017. \n",
    "# The data download links are saved to the dataset_urls.txt file.\n",
    "\n",
    "train_truth_url, train_url, test_url = open(\"dataset_urls.txt\").readlines()\n",
    "\n",
    "train_h5 = wget.download(train_url)\n",
    "train_truth_h5 = wget.download(train_truth_url)\n",
    "test_h5 = wget.download(test_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xOMngnOFESM",
    "outputId": "1077d025-6fa4-4400-875c-729f117e3cdc"
   },
   "outputs": [],
   "source": [
    "!unzip 'CVPPP2017_training_images.zip'\n",
    "!unzip 'CVPPP2017_training_truth.zip'\n",
    "!unzip 'CVPPP2017_testing_images.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHak6-ACCJzn"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "groups = ['A1', 'A2', 'A3']\n",
    "\n",
    "train_filename = 'CVPPP2017_training_images.h5'\n",
    "train_imgs = []\n",
    "with h5py.File(train_filename, \"r\") as f:\n",
    "    for group in groups:\n",
    "        for file_name in list(f[group]):\n",
    "            train_imgs.append(np.moveaxis(np.array(f[group][file_name]['rgb'])[:, :, :3], [2], [0]))\n",
    "\n",
    "\n",
    "train_truth_filename = 'CVPPP2017_training_truth.h5'\n",
    "train_counts = []\n",
    "train_masks = []\n",
    "with h5py.File(train_truth_filename, \"r\") as f:\n",
    "    for group in groups:\n",
    "        for file_name in list(f[group]):\n",
    "            train_counts.append(np.array(f[group][file_name]['count']))\n",
    "            train_masks.append(np.clip(np.array(f[group][file_name]['label']), 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85HIftr_GeiM",
    "outputId": "cadd58c7-f26c-4165-cc98-0c553c3fca33"
   },
   "outputs": [],
   "source": [
    "print(len(train_imgs), len(train_counts), len(train_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "Luvzhq6tFXR6",
    "outputId": "de9e7f8b-9f0e-4846-f27c-1e99da153791"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "\n",
    "axarr[0].imshow(np.moveaxis(train_imgs[0], [0], [2]))\n",
    "axarr[1].imshow(train_masks[0], cmap='gray')\n",
    "\n",
    "f.set_figheight(10)\n",
    "f.set_figwidth(10)\n",
    "\n",
    "print('The number of leafs:', train_counts[0])\n",
    "plt.imshow(train_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_to(x, stride):\n",
    "    h, w = x.shape[-2:]\n",
    "\n",
    "    if h % stride > 0:\n",
    "        new_h = h + stride - h % stride\n",
    "    else:\n",
    "        new_h = h\n",
    "    if w % stride > 0:\n",
    "        new_w = w + stride - w % stride\n",
    "    else:\n",
    "        new_w = w\n",
    "\n",
    "    lh, uh = int((new_h - h) / 2), (new_h - h) - int((new_h - h) / 2)\n",
    "    lw, uw = int((new_w - w) / 2), (new_w - w) - int((new_w - w) / 2)\n",
    "    pads = (lw, uw, lh, uh)\n",
    "\n",
    "    out = F.pad(x, pads, \"constant\", 0)\n",
    "\n",
    "    return out, pads\n",
    "\n",
    "def unpad(x, pad):\n",
    "    if pad[2] + pad[3] > 0:\n",
    "        x = x[:, :, pad[2]: -pad[3], :]\n",
    "    if pad[0] + pad[1] > 0:\n",
    "        x = x[:, :, :, pad[0]: -pad[1]]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "vEcD1P1r678n"
   },
   "outputs": [],
   "source": [
    "## Augmentation to be implemented\n",
    "\n",
    "# Augmentation types from paper:\n",
    "# (1) Random-Rotate with an interval of 90 to increase the network invariance to slight angular changes; \n",
    "# (2) Flip: horizontal, vertical, and horizontal+ vertical; \n",
    "# (3) Resize the images to increase the network invariance to different image resolutions; \n",
    "# (4) Gamma transform to extend the data by changing the image greyscale; \n",
    "# (5) Random-Brightness: the clarity of object depends on scene lighting and camera sensitivity,\n",
    "# (6) Random change in the contrast range to increase the network invariance to shadows and improve the network performance in low light conditions; \n",
    "# (7) Hue Saturation Brightness (HSV): changes in colour channels, degree of lightness or darkness of a colour; and \n",
    "# (8) Normalise a characteristic linear transformation which scales a specific range of data values retaining the original data distribution\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "\n",
    "# Padding is needed as there will be skip connections concatenations\n",
    "PAD_BASE = 16\n",
    "\n",
    "aug_transforms = [\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomApply([transforms.RandomGrayscale(p=0.1)], p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]\n",
    "\n",
    "aug_transforms = transforms.Compose(aug_transforms + [transforms.ToTensor()])\n",
    "\n",
    "class LeafDataset(Dataset):\n",
    "    def __init__(self, imgs, masks, counts, aug_transforms=None):\n",
    "        super().__init__()\n",
    "        self.images = imgs\n",
    "        self.masks = masks\n",
    "        self.counts = counts\n",
    "\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        seed = random.randint(0, 2**32)\n",
    "        self._set_seed(seed)\n",
    "        aug_img = self.transforms(self.images[idx])\n",
    "        self._set_seed(seed)\n",
    "        aug_mask = self.transforms(self.masks[idx])\n",
    "        return pad_to(aug_img, PAD_BASE)[0], self.counts[idx], pad_to(aug_mask, PAD_BASE)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "XwHpvBtcJg3H"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_size = 0.8\n",
    "dataset_len = len(train_imgs)\n",
    "train_len = int(dataset_len * train_size)\n",
    "\n",
    "train_lst = list(zip(train_imgs, train_counts, train_masks))[:train_len]\n",
    "test_lst= list(zip(train_imgs, train_counts, train_masks))[train_len:]\n",
    "\n",
    "train_dataset = LeafDataset(train_imgs[:train_len], train_masks[:train_len], train_counts[:train_len], aug_transforms)\n",
    "test_dataset = LeafDataset(train_imgs[train_len:], train_imgs[train_len:], train_imgs[train_len:], transforms.ToTensor())\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzstblOZzPei"
   },
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "BxkSSH8ApWKQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxec5NWA1J7x",
    "outputId": "cdb90b5a-3b54-492c-c18a-e92de5f850f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Working on {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158,
     "referenced_widgets": [
      "d1611e5e08e543559b4100b7a4103d76",
      "784dd9d7cef54dd48ce73ec3c1b890d3",
      "a3f2be51a22246089b6303cd164fb5d8",
      "0176ae1cacf643dba6cc59ece3bd948c",
      "7bf22d562a1f49e69025ca948e233325",
      "43975737083c4bcfa169cfff688a1491",
      "8a1e43a7ad974039b37d61fac2d43323",
      "a90e8702f2e44b1b80d392872ecdbc13",
      "a034bb304f174633b7b0013d29e7bc25",
      "95cb4b30c2d14b69834e81f5027df81e",
      "5be1c673dadf4c9ba376bd63ffded40d"
     ]
    },
    "id": "Gyf9AtLlu7tn",
    "outputId": "523b99e6-614a-42da-db13-e17a6f7b0fcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1611e5e08e543559b4100b7a4103d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First stream network\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "6ZhCQ3ulOHCj"
   },
   "outputs": [],
   "source": [
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We will stack the 3 channels of the input image with the mask. Thus, 4 channels in total.\n",
    "resnet50.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=2, padding=3, bias=False)\n",
    "for param in resnet50.conv1.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# We will solve regression problem\n",
    "resnet50.fc = torch.nn.Linear(2048, 1)\n",
    "for param in resnet50.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "4bxzgoLuvEKM"
   },
   "outputs": [],
   "source": [
    "# Second stream network\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_conv1 = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                                       nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU())\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 256 -> 128\n",
    "        \n",
    "        self.enc_conv2 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "                                       nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU())\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 128 -> 64\n",
    "\n",
    "        self.enc_conv3 = nn.Sequential(nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "                                       nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "                                       nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU())\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 64 -> 32\n",
    "\n",
    "        self.enc_conv4 = nn.Sequential(nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "                                       nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "                                       nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU())\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)  # 32 -> 16\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottleneck_conv = nn.Sequential(nn.Conv2d(512, 1024, 3, padding=1), nn.BatchNorm2d(1024), nn.ReLU(),\n",
    "                                       nn.Conv2d(1024, 1024, 3, padding=1), nn.BatchNorm2d(1024), nn.ReLU())\n",
    "\n",
    "        # decoder (upsampling)\n",
    "        self.upsample1 = nn.ConvTranspose2d(1024, 512, 2, stride=2)  # 16 -> 32\n",
    "        self.dec_conv1 = nn.Sequential(nn.Conv2d(1024, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "                                       nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU())\n",
    "        \n",
    "        self.upsample2 = nn.ConvTranspose2d(512, 256, 2, stride=2)  # 16 -> 32\n",
    "        self.dec_conv2 = nn.Sequential(nn.Conv2d(512, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "                                       nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU())\n",
    "        \n",
    "        self.upsample3 = nn.ConvTranspose2d(256, 128, 2, stride=2)  # 32 -> 64\n",
    "        self.dec_conv3 = nn.Sequential(nn.Conv2d(256, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "                                       nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU())\n",
    "\n",
    "        self.upsample4 = nn.ConvTranspose2d(128, 64, 2, stride=2)   # 64 -> 128\n",
    "        self.dec_conv4 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                                       nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                                       nn.Conv2d(64, 1, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        e1 = self.enc_conv1(x)\n",
    "        e2 = self.enc_conv2(self.pool1(e1))\n",
    "        e3 = self.enc_conv3(self.pool2(e2))\n",
    "        e4 = self.enc_conv4(self.pool3(e3))\n",
    "\n",
    "        # bottleneck\n",
    "        b = self.bottleneck_conv(self.pool4(e4))\n",
    "\n",
    "        # decoder\n",
    "        d1 = self.dec_conv1(torch.concat([e4, self.upsample1(b)], 1))\n",
    "        d2 = self.dec_conv2(torch.concat([e3, self.upsample2(d1)], 1))\n",
    "        d3 = self.dec_conv3(torch.concat([e2, self.upsample3(d2)], 1))\n",
    "        d4 = self.dec_conv4(torch.concat([e1, self.upsample4(d3)], 1))\n",
    "        return torch.sigmoid(d4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLKIWXzWzbYn"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "PEFrCTOZvpQC"
   },
   "outputs": [],
   "source": [
    "def loss_function(y_real, y_pred):\n",
    "    # dice loss\n",
    "    num = (y_real * y_pred).view(4, 1, -1).sum(2)\n",
    "    den = (y_real + y_pred).view(4, 1, -1).sum(2)\n",
    "    res = -2 * num / den\n",
    "    dice = res.mean()\n",
    "\n",
    "    # bce loss\n",
    "    bce = (-0.5 * y_real * torch.log(y_pred)).sum(2).mean()\n",
    "    return dice + bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "X2YiT1oO0E5Q"
   },
   "outputs": [],
   "source": [
    "def train_segmentation(model_segm, model_count, epochs, loss_fn, opt, data_tr, data_val):\n",
    "    \n",
    "    train_segm_losses = []\n",
    "    train_count_losses = []\n",
    "    val_segm_losses = []\n",
    "    val_count_losses = []\n",
    "\n",
    "    for epoch in epochs:\n",
    "        model_segm.train()\n",
    "        model_count.train()\n",
    "        for imgs, counts, masks in tqdm(data_tr):\n",
    "            imgs, counts, masks = imgs.to(device), counts.to(device), masks.to(device)\n",
    "            \n",
    "            # Train segmentation\n",
    "            opt.zero_grad()\n",
    "            output = model_segm(imgs)\n",
    "            loss = loss_fn(output, masks)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_segm_losses.append(loss.item())\n",
    "\n",
    "            # Train counts\n",
    "            opt.zero_grad()\n",
    "            output = model_count(torch.stack((imgs, masks)))\n",
    "            loss = loss_fn(output, counts)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_count_losses.append(loss.item())\n",
    "\n",
    "        \n",
    "        model_segm.eval()\n",
    "        model_count.eval()\n",
    "        with torch.no_grad():        \n",
    "            for imgs, counts, masks in tqdm(data_val):\n",
    "                imgs, counts, masks = imgs.to(device), counts.to(device), masks.to(device)\n",
    "                # Evaluate segmentation\n",
    "                output = model_segm(imgs)\n",
    "                loss = loss_fn(output, masks)\n",
    "                val_segm_losses.append(loss.item())\n",
    "\n",
    "                # Evaluate counts\n",
    "                output = model_count(torch.stack((imgs, masks)))\n",
    "                loss = loss_fn(output, counts)\n",
    "                val_count_losses.append(loss.item())\n",
    "        print(f'Epoch: {epoch}, train loss: {train_segm_losses[-1], train_count_losses[-1]}, val loss: {val_segm_losses[-1], val_count_losses[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZ0_lKHd3CBV"
   },
   "outputs": [],
   "source": [
    "model_segm = UNet().to(device)\n",
    "model_count = resnet50.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snvz1tIZ7GKV"
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "epochs = 100\n",
    "optimizer = optim.Adam(lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "train(model_segm, model_count, epochs, loss_function, optimizer, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGJFFy9izfst"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAy3iu1FzsxZ"
   },
   "outputs": [],
   "source": [
    "## eval functions to be added\n",
    "\n",
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    # You can comment out this line if you are passing tensors of equal shape\n",
    "    # But if you are passing output from UNet or something it will most probably\n",
    "    # be with the BATCH x 1 x H x W shape\n",
    "    \n",
    "    outputs = outputs.squeeze(1).byte()  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    labels = labels.squeeze(1).byte()\n",
    "    SMOOTH = 1e-8\n",
    "    union = (outputs | labels).float().sum((1, 2))         # Will be zero if both are 0\n",
    "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    \n",
    "    return thresholded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hoZZaIvN0kk"
   },
   "source": [
    "## Build web demo with Gradio library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXmF_4IJN0Qn"
   },
   "outputs": [],
   "source": [
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "elHOMr2jN_SU",
    "outputId": "2e451e19-37cb-4f5f-90be-d5f111ec3d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "(async (port, path, width, height, cache, element) => {\n",
       "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
       "                            return;\n",
       "                        }\n",
       "                        element.appendChild(document.createTextNode(''));\n",
       "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
       "\n",
       "                        const external_link = document.createElement('div');\n",
       "                        external_link.innerHTML = `\n",
       "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
       "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
       "                                    https://localhost:${port}${path}\n",
       "                                </a>\n",
       "                            </div>\n",
       "                        `;\n",
       "                        element.appendChild(external_link);\n",
       "\n",
       "                        const iframe = document.createElement('iframe');\n",
       "                        iframe.src = new URL(path, url).toString();\n",
       "                        iframe.height = height;\n",
       "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
       "                        iframe.width = width;\n",
       "                        iframe.style.border = 0;\n",
       "                        element.appendChild(iframe);\n",
       "                    })(7862, \"/\", \"100%\", 500, false, window.element)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "\n",
    "def predict_leaf_number(img):\n",
    "    # preprocessing = ...\n",
    "    return model_count(torch.stack((img, model_segm(img))))\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    gr.Markdown(\"# Leaf counter\")\n",
    "\n",
    "    with gr.Row():\n",
    "        im = gr.Image()\n",
    "        txt = gr.Textbox(value='', label=\"Leaf number\")\n",
    "\n",
    "    btn = gr.Button(value=\"Mirror Image\")\n",
    "    btn.click(predict_leaf_number, inputs=[im], outputs=[txt])\n",
    "\n",
    "    gr.Markdown(\"## Image Examples\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0176ae1cacf643dba6cc59ece3bd948c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95cb4b30c2d14b69834e81f5027df81e",
      "placeholder": "​",
      "style": "IPY_MODEL_5be1c673dadf4c9ba376bd63ffded40d",
      "value": " 97.8M/97.8M [00:01&lt;00:00, 61.5MB/s]"
     }
    },
    "43975737083c4bcfa169cfff688a1491": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5be1c673dadf4c9ba376bd63ffded40d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "784dd9d7cef54dd48ce73ec3c1b890d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43975737083c4bcfa169cfff688a1491",
      "placeholder": "​",
      "style": "IPY_MODEL_8a1e43a7ad974039b37d61fac2d43323",
      "value": "100%"
     }
    },
    "7bf22d562a1f49e69025ca948e233325": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a1e43a7ad974039b37d61fac2d43323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95cb4b30c2d14b69834e81f5027df81e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a034bb304f174633b7b0013d29e7bc25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a3f2be51a22246089b6303cd164fb5d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90e8702f2e44b1b80d392872ecdbc13",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a034bb304f174633b7b0013d29e7bc25",
      "value": 102530333
     }
    },
    "a90e8702f2e44b1b80d392872ecdbc13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1611e5e08e543559b4100b7a4103d76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_784dd9d7cef54dd48ce73ec3c1b890d3",
       "IPY_MODEL_a3f2be51a22246089b6303cd164fb5d8",
       "IPY_MODEL_0176ae1cacf643dba6cc59ece3bd948c"
      ],
      "layout": "IPY_MODEL_7bf22d562a1f49e69025ca948e233325"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
